#!/usr/bin/env python3
"""
PySpark job for processing academic papers metadata.
This job reads metadata from S3 (created by the PDF ingestion service) and processes it.
No PDF downloads happen in this job - that's handled by the separate Python service.
"""

from pyspark.sql import SparkSession
import pyspark.sql.functions as F
import pyspark.sql.types as T
import logging
from config import *

# Configure logging
logging.basicConfig(level=getattr(logging, LOG_LEVEL), format=LOG_FORMAT)
logger = logging.getLogger(__name__)

def create_spark_session():
    """Create and configure Spark session with Iceberg support"""
    builder = SparkSession.builder.appName("AcademicPapersProcessing")
    
    # Apply all Spark configurations
    for key, value in SPARK_CONFIGS.items():
        builder = builder.config(key, value)
    
    return builder.getOrCreate()

def get_paper_schema():
    """Define schema for academic papers"""
    return T.StructType([
        T.StructField("id", T.StringType(), False),  # Already generated by PDF service
        T.StructField("doi", T.StringType(), True),
        T.StructField("title", T.StringType(), False),
        T.StructField("authors", T.StringType(), False),
        T.StructField("abstract", T.StringType(), True),
        T.StructField("full_text", T.StringType(), True),
        T.StructField("journal", T.StringType(), True),
        T.StructField("publication_date", T.DateType(), False),
        T.StructField("keywords", T.StringType(), True),
        T.StructField("pdf_url", T.StringType(), True),
        T.StructField("pdf_s3_path", T.StringType(), True),  # S3 path to downloaded PDF
        T.StructField("source", T.StringType(), True),
        T.StructField("ingestion_timestamp", T.TimestampType(), True)
    ])

def clean_text_field(df, column_name):
    """Clean and validate text fields"""
    return df.withColumn(
        column_name,
        F.when(
            (F.col(column_name).isNotNull()) & 
            (F.trim(F.col(column_name)) != "") & 
            (F.col(column_name) != "null") & 
            (F.col(column_name) != "NULL"),
            F.trim(F.col(column_name))
        ).otherwise(None)
    )

def validate_doi(df):
    """Basic DOI validation"""
    return df.withColumn(
        "doi_valid",
        F.when(
            F.col("doi").isNotNull() & 
            F.col("doi").rlike("^10\\.\\d{4,}/.+"),
            F.lit(True)
        ).otherwise(F.lit(False))
    )

def apply_data_quality_checks(df):
    """Apply comprehensive data quality checks"""
    # Filter out records with invalid titles
    df = df.filter(
        (F.col("title").isNotNull()) & 
        (F.trim(F.col("title")) != "") &
        (F.length(F.trim(F.col("title"))) >= MIN_TITLE_LENGTH)
    )
    
    # Truncate overly long text fields
    df = df.withColumn(
        "abstract",
        F.when(
            F.col("abstract").isNotNull() & 
            (F.length(F.col("abstract")) > MAX_ABSTRACT_LENGTH),
            F.substring(F.col("abstract"), 1, MAX_ABSTRACT_LENGTH)
        ).otherwise(F.col("abstract"))
    )
    
    df = df.withColumn(
        "full_text",
        F.when(
            F.col("full_text").isNotNull() & 
            (F.length(F.col("full_text")) > MAX_FULL_TEXT_LENGTH),
            F.substring(F.col("full_text"), 1, MAX_FULL_TEXT_LENGTH)
        ).otherwise(F.col("full_text"))
    )
    
    return df

def load_source_data(spark, source_name, source_config):
    """Load metadata from S3 (created by PDF ingestion service)"""
    if not source_config.get("enabled", True):
        logger.info(f"Skipping disabled source: {source_name}")
        return None
    
    # Read from the metadata files created by the PDF ingestion service
    s3_path = f"s3://{S3_BUCKET}/raw_metadata/{source_name}/*.{source_config.get('format', 'parquet')}"
    logger.info(f"Loading metadata from {source_name}: {s3_path}")
    
    try:
        file_format = source_config.get("format", "parquet")
        
        if file_format == "parquet":
            df = spark.read.schema(get_paper_schema()).parquet(s3_path)
        elif file_format == "json":
            df = spark.read.schema(get_paper_schema()).json(s3_path)
        else:
            raise ValueError(f"Unsupported file format: {file_format}")
        
        # Add source identifier (in case it's not in the data)
        df = df.withColumn("source", F.lit(source_name))
        
        # Clean text fields
        text_fields = ["title", "abstract", "full_text", "authors", "keywords"]
        for field in text_fields:
            df = clean_text_field(df, field)
        
        # Apply data quality checks
        df = apply_data_quality_checks(df)
        
        # Validate DOI
        df = validate_doi(df)
        
        record_count = df.count()
        logger.info(f"Loaded {record_count} records from {source_name}")
        
        if record_count == 0:
            logger.warning(f"No valid records found for {source_name}")
            return None
            
        return df
        
    except Exception as e:
        logger.error(f"Error loading data from {source_name}: {str(e)}")
        return None

def write_to_iceberg_raw_table(df, source_name):
    """Write data directly to source-specific Iceberg raw table"""
    if df is None or df.count() == 0:
        logger.warning(f"No data to write for {source_name}")
        return
    
    # Add partitioning columns
    df_partitioned = df.withColumn("year", F.year(F.col("publication_date"))) \
                       .withColumn("month", F.month(F.col("publication_date"))) \
                       .withColumn("day", F.dayofmonth(F.col("publication_date"))) \
                       .withColumn("ingestion_timestamp", F.current_timestamp())
    
    iceberg_path = f"s3://{S3_BUCKET}/iceberg/{source_name}_raw"
    
    logger.info(f"Writing {df_partitioned.count()} records to {iceberg_path}")
    
    df_partitioned.write.format("iceberg").mode("append").partitionBy("year", "month", "day").save(iceberg_path)

def merge_to_master_table(spark, source_dfs):
    """Merge all source data into the master table"""
    if not source_dfs:
        logger.warning("No data to merge")
        return
    
    # Combine all source DataFrames
    combined_df = source_dfs[0]
    for df in source_dfs[1:]:
        combined_df = combined_df.unionByName(df, allowMissingColumns=True)
    
    # Select relevant columns
    master_columns = [
        "id", "doi", "title", "authors", "abstract", 
        "full_text", "journal", "publication_date", "keywords", 
        "pdf_url", "pdf_s3_path", "source", "doi_valid"
    ]
    
    combined_df = combined_df.select(*master_columns)
    
    # Add partition columns
    combined_df = combined_df.withColumn("year", F.year(F.col("publication_date"))) \
                             .withColumn("month", F.month(F.col("publication_date"))) \
                             .withColumn("day", F.dayofmonth(F.col("publication_date"))) \
                             .withColumn("ingestion_timestamp", F.current_timestamp())
    
    # Create temp view for merge
    combined_df.createOrReplaceTempView("staging_papers")
    
    # Merge into master table
    merge_sql = f"""
    MERGE INTO {MASTER_TABLE_NAME} AS master
    USING staging_papers AS staging
    ON master.id = staging.id
    AND master.year = staging.year
    AND master.month = staging.month
    AND master.day = staging.day
    WHEN MATCHED THEN UPDATE SET
      master.doi = COALESCE(staging.doi, master.doi),
      master.title = COALESCE(staging.title, master.title),
      master.authors = COALESCE(staging.authors, master.authors),
      master.abstract = COALESCE(staging.abstract, master.abstract),
      master.full_text = COALESCE(staging.full_text, master.full_text),
      master.journal = COALESCE(staging.journal, master.journal),
      master.publication_date = COALESCE(staging.publication_date, master.publication_date),
      master.keywords = COALESCE(staging.keywords, master.keywords),
      master.pdf_url = COALESCE(staging.pdf_url, master.pdf_url),
      master.pdf_s3_path = COALESCE(staging.pdf_s3_path, master.pdf_s3_path),
      master.source = COALESCE(staging.source, master.source),
      master.doi_valid = COALESCE(staging.doi_valid, master.doi_valid),
      master.ingestion_timestamp = staging.ingestion_timestamp
    WHEN NOT MATCHED THEN INSERT *
    """
    
    logger.info("Executing merge into master table...")
    spark.sql(merge_sql)
    
    # Log results
    master_count = spark.sql(f"SELECT COUNT(*) as count FROM {MASTER_TABLE_NAME}").collect()[0]["count"]
    logger.info(f"Master table now contains {master_count} total records")

def main():
    """Main processing pipeline"""
    spark = create_spark_session()
    
    logger.info("Starting academic papers processing pipeline...")
    
    # Load raw data from all enabled sources (metadata from PDF ingestion service)
    raw_dataframes = {}
    for source_name, source_config in DATA_SOURCES.items():
        df = load_source_data(spark, source_name, source_config)
        if df is not None:
            raw_dataframes[source_name] = df
    
    if not raw_dataframes:
        logger.error("No data sources loaded successfully")
        return
    
    # Write each source to its own raw table
    for source_name, df in raw_dataframes.items():
        write_to_iceberg_raw_table(df, source_name)
    
    # Filter for recent data
    cutoff_date = F.date_sub(F.current_date(), PARTITION_LOOKBACK_DAYS)
    logger.info(f"Processing data from {cutoff_date} onwards")
    
    filtered_dfs = []
    for source_name, df in raw_dataframes.items():
        # Filter for recent data
        filtered_df = df.filter(F.col("publication_date") >= cutoff_date)
        if filtered_df.count() > 0:
            filtered_dfs.append(filtered_df)
            logger.info(f"Filtered {filtered_df.count()} records from {source_name}")
    
    # Merge to master table
    if filtered_dfs:
        merge_to_master_table(spark, filtered_dfs)
    else:
        logger.warning("No recent data found to process")
    
    logger.info("Processing pipeline completed")
    spark.stop()

if __name__ == "__main__":
    main() 